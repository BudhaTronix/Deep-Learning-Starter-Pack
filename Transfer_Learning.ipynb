{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsTmvhizAfSa"
   },
   "source": [
    "Name : Budhaditya Mukhopadhyay (229960) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXuNJQ5lxLgs"
   },
   "source": [
    "# **TASK 3 : Transfer Learning**\n",
    "\n",
    "In this task, you will implement different strategies on doing transfer learning.\n",
    "You will also investigate how well transfer learning is applicable to inputs of different data distributions.\n",
    "\n",
    "3.1 same distribution\n",
    "Split the MNIST dataset, such that 7 classes are the source domain and the other 3 classes are the target domain.\n",
    "Start with the source domain data set and train a model on it.\n",
    "From the trained model, cut off the final classification layer and replace it with a new layer for the target domain classes. For now, keep all layers trainable.\n",
    "Now train this model on the target domain data set. For comparison, also train a freshly initialized model on the target domain data set.\n",
    "\n",
    "How well did the transfer learning work, compared to training on the target data set from scratch?\n",
    "You can e.g. compare:\n",
    "\n",
    "achieved accuracy\n",
    "steps until convergence\n",
    "generalization\n",
    "3.2 different distribution\n",
    "Now we will work with MNIST as source domain and FMNIST as target domain. (This is convenient, as they already have the same input shape.)\n",
    "Similar to 3.1, first train a model on the MNIST and use the pre-trained model as initialization for a model trained on FMNIST.\n",
    "Compare this transfer learned model with a model trained on FMNIST from scratch using the measures from 3.1.\n",
    "\n",
    "Repeat the experiment with FMNIST as source and MNIST as target!\n",
    "Does transfer learning work better or worse this time?\n",
    "\n",
    "3.3 Compare diffferent ways to do the transfer learning\n",
    "Transfer learning is done with different strategies.\n",
    "You can for example investigate:\n",
    "\n",
    "freezing all pre-trained layers (i.e. set trainable to False)\n",
    "freezing only some bottom layers\n",
    "replacing (or reinitializing) multiple top layers\n",
    "replacing the source classification layer by multiple new ones for the target domain\n",
    "This is particularly meaningful, if the target domain is more complex than the source domain.\n",
    "Do you observe any differences in the final result using different ways of going from target to source?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpSvQsl8xVao"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2867,
     "status": "ok",
     "timestamp": 1600127167344,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "INbydiJ5LMdU",
    "outputId": "a2e5bfe0-852d-480f-d27b-fb8fb82831e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2860,
     "status": "ok",
     "timestamp": 1600127167346,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "umUYVMw5LRl6",
    "outputId": "020992fe-7c75-4099-c820-a73912a654bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ot4Mwj2CxZyv"
   },
   "source": [
    "## 3.1 same distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsHAX7PZLa9V"
   },
   "outputs": [],
   "source": [
    "# Domain Separation, i.e. [0,6] and [7,9]\n",
    "d1_train_index = []\n",
    "d1_test_index = []\n",
    "\n",
    "d2_train_index = []\n",
    "d2_test_index = []\n",
    "\n",
    "for idx in range(len(Y_train)):\n",
    "    if Y_train[idx] < 7:\n",
    "        d1_train_index.append(idx)\n",
    "    else:\n",
    "        d2_train_index.append(idx)\n",
    "\n",
    "for idx in range(len(Y_test)):\n",
    "    if Y_test[idx] < 7:\n",
    "        d1_test_index.append(idx)\n",
    "    else:\n",
    "        d2_test_index.append(idx)\n",
    "\n",
    "# Domain 1\n",
    "d1_train_x = X_train[d1_train_index]\n",
    "d1_train_y = Y_train[d1_train_index]\n",
    "\n",
    "d1_test_x = X_test[d1_test_index]\n",
    "d1_test_y = Y_test[d1_test_index]\n",
    "\n",
    "\n",
    "# Domain 2\n",
    "d2_train_x = X_train[d2_train_index]\n",
    "d2_train_y = Y_train[d2_train_index]-7\n",
    "\n",
    "d2_test_x = X_test[d2_test_index]\n",
    "d2_test_y = Y_test[d2_test_index]-7\n",
    "\n",
    "# dataset.train_labels = dataset.train_labels[idx]\n",
    "# dataset.train_data = dataset.train_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Mak59E3Lik8"
   },
   "outputs": [],
   "source": [
    "d1_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (d1_train_x.reshape([-1, 28, 28, 1]).astype(np.float32) / 255, d1_train_y.astype(np.int32)))\n",
    "\n",
    "d1_dataset = d1_dataset.shuffle(buffer_size = 40000).batch(128).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LI4t1AerLlTq"
   },
   "outputs": [],
   "source": [
    "d2_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (d2_train_x.reshape([-1, 28, 28, 1]).astype(np.float32) / 255, d2_train_y.astype(np.int32)))\n",
    "\n",
    "d2_dataset = d2_dataset.shuffle(buffer_size = 40000).batch(128).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9198,
     "status": "ok",
     "timestamp": 1600127173718,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "6-mzJF_gLnyl",
    "outputId": "d92b1187-4124-436f-f492-044ca29ee10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa6f027fd68>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(d1_train_y[100])\n",
    "plt.imshow(d1_train_x[100], cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o4zvhWoLsoq"
   },
   "outputs": [],
   "source": [
    "train_steps = 1500\n",
    "\n",
    "layer_list = [ tf.keras.layers.Conv2D(filters = 64,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "              tf.keras.layers.Conv2D(filters = 64,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              tf.keras.layers.Dropout(0.25),\n",
    "              \n",
    "              tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              tf.keras.layers.Dropout(0.25),\n",
    "              \n",
    "               tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "                    \n",
    "              \n",
    "              tf.keras.layers.Conv2D(filters=256,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              \n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(7,activation='softmax')]  \n",
    "model = tf.keras.Sequential(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7p5ALwaDLw8o"
   },
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(0.001)\n",
    "# from_logits = True!! #neverforget\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52695,
     "status": "ok",
     "timestamp": 1600127217241,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "hBS7D8oRL495",
    "outputId": "471372af-397f-461e-a4d9-d2f28983cbbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.945624589920044 Accuracy: 0.2421875\n",
      "Loss: 1.1979749202728271 Accuracy: 0.6174218654632568\n",
      "Loss: 1.1753778457641602 Accuracy: 0.9644531011581421\n",
      "Loss: 1.177947998046875 Accuracy: 0.9735156297683716\n",
      "Loss: 1.180203914642334 Accuracy: 0.978903591632843\n",
      "Loss: 1.1732628345489502 Accuracy: 0.9769531488418579\n",
      "Loss: 1.2030490636825562 Accuracy: 0.9784374833106995\n",
      "Loss: 1.1816561222076416 Accuracy: 0.9808642268180847\n",
      "Loss: 1.2110333442687988 Accuracy: 0.9767187237739563\n",
      "Loss: 1.1813955307006836 Accuracy: 0.9825000166893005\n",
      "Loss: 1.2513420581817627 Accuracy: 0.9767076969146729\n",
      "Loss: 1.2045217752456665 Accuracy: 0.9669530987739563\n",
      "Loss: 1.1810489892959595 Accuracy: 0.9649999737739563\n",
      "Loss: 1.3284459114074707 Accuracy: 0.9516406059265137\n",
      "Loss: 1.2201097011566162 Accuracy: 0.953807532787323\n",
      "Loss: 1.1732347011566162 Accuracy: 0.948437511920929\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(d1_dataset):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3n15ebPJM-jc"
   },
   "source": [
    "Changing the Last Layer\n",
    "\n",
    "Training on target domain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyJTLcUoL61x"
   },
   "outputs": [],
   "source": [
    "layers_minus_softmax = model.layers[-2].output\n",
    "output_with_replaced_softmax_layer = tf.keras.layers.Dense(3, activation='softmax')(layers_minus_softmax)\n",
    "model = Model(inputs=model.inputs, outputs=output_with_replaced_softmax_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88632,
     "status": "ok",
     "timestamp": 1600127253197,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "npcnWRHRNLZz",
    "outputId": "651bc4b9-ef8c-43e4-bbe3-f30ef530f0f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2613710165023804 Accuracy: 0.2890625\n",
      "Loss: 1.23113214969635 Accuracy: 0.3492187559604645\n",
      "Loss: 1.20769464969635 Accuracy: 0.34139806032180786\n",
      "Loss: 1.21550714969635 Accuracy: 0.3487272560596466\n",
      "Loss: 1.21550714969635 Accuracy: 0.34867188334465027\n",
      "Loss: 1.23113214969635 Accuracy: 0.3450232446193695\n",
      "Loss: 1.23113214969635 Accuracy: 0.34904247522354126\n",
      "Loss: 1.19988214969635 Accuracy: 0.34468749165534973\n",
      "Loss: 1.21550714969635 Accuracy: 0.34967294335365295\n",
      "Loss: 1.18425714969635 Accuracy: 0.3443927764892578\n",
      "Loss: 1.19988214969635 Accuracy: 0.3498305678367615\n",
      "Loss: 1.19988214969635 Accuracy: 0.34351563453674316\n",
      "Loss: 1.22331964969635 Accuracy: 0.3456537127494812\n",
      "Loss: 1.16081964969635 Accuracy: 0.3452596664428711\n",
      "Loss: 1.20769464969635 Accuracy: 0.34812501072883606\n",
      "Loss: 1.20769464969635 Accuracy: 0.3508550822734833\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(d2_dataset):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3yqOAtuCwId"
   },
   "source": [
    "### Observation for 3.1\n",
    "\n",
    "How well did the transfer learning work, compared to training on the target data set from scratch?\n",
    "\n",
    "Ans - It did not perform that well, we are getting an accuracy of 35%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXD7JzsGNd_0"
   },
   "source": [
    "## 3.2 Different Distribution\n",
    "\n",
    "Source domain = MNIST target domain= Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrO70AwcNklr"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSKUtHk5NzfQ"
   },
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train.reshape([-1, 28, 28, 1]).astype(np.float32) / 255, Y_train.astype(np.int32)))\n",
    "\n",
    "data1 = data.shuffle(buffer_size = 40000).batch(128).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Piz_OmPaN1TN"
   },
   "outputs": [],
   "source": [
    "train_steps = 1500\n",
    "\n",
    "layer_list = [ tf.keras.layers.Conv2D(filters = 64,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "              tf.keras.layers.Conv2D(filters = 64,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              tf.keras.layers.Dropout(0.5),\n",
    "              \n",
    "              tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              tf.keras.layers.Dropout(0.25),\n",
    "              \n",
    "               tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "                    \n",
    "              \n",
    "              tf.keras.layers.Conv2D(filters=256,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              \n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(7,activation='softmax')]  \n",
    "model = tf.keras.Sequential(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 125149,
     "status": "ok",
     "timestamp": 1600127289747,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "jICyrOBaN2_1",
    "outputId": "0437e82d-ea8a-435f-d1bc-2e5e44c329d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan Accuracy: 0.15625\n",
      "Loss: nan Accuracy: 0.10195312649011612\n",
      "Loss: nan Accuracy: 0.09593749791383743\n",
      "Loss: nan Accuracy: 0.10101562738418579\n",
      "Loss: nan Accuracy: 0.09742187708616257\n",
      "Loss: nan Accuracy: 0.09727443754673004\n",
      "Loss: nan Accuracy: 0.09851562231779099\n",
      "Loss: nan Accuracy: 0.099609375\n",
      "Loss: nan Accuracy: 0.09546875208616257\n",
      "Loss: nan Accuracy: 0.10296875238418579\n",
      "Loss: nan Accuracy: 0.09946741908788681\n",
      "Loss: nan Accuracy: 0.09554687142372131\n",
      "Loss: nan Accuracy: 0.09898437559604645\n",
      "Loss: nan Accuracy: 0.10296875238418579\n",
      "Loss: nan Accuracy: 0.09515625238418579\n",
      "Loss: nan Accuracy: 0.0973527580499649\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(data1):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 126748,
     "status": "ok",
     "timestamp": 1600127291349,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "WZsj3B7lN4i_",
    "outputId": "1976f0b8-b39f-4146-f8a9-be705ccba238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5urNKX1D_SxP"
   },
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train.reshape([-1, 28, 28, 1]).astype(np.float32) / 255, Y_train.astype(np.int32)))\n",
    "\n",
    "data2 = data.shuffle(buffer_size = 40000).batch(128).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 163304,
     "status": "ok",
     "timestamp": 1600127327911,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "BbUxh4I8_UYv",
    "outputId": "565b6765-ab6a-43fc-a9e6-f37d63df8aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan Accuracy: 0.09375\n",
      "Loss: nan Accuracy: 0.0949999988079071\n",
      "Loss: nan Accuracy: 0.09976562857627869\n",
      "Loss: nan Accuracy: 0.10187499970197678\n",
      "Loss: nan Accuracy: 0.09968750178813934\n",
      "Loss: nan Accuracy: 0.0958646610379219\n",
      "Loss: nan Accuracy: 0.09734375029802322\n",
      "Loss: nan Accuracy: 0.09726562350988388\n",
      "Loss: nan Accuracy: 0.10398437827825546\n",
      "Loss: nan Accuracy: 0.09929687529802322\n",
      "Loss: nan Accuracy: 0.0943765640258789\n",
      "Loss: nan Accuracy: 0.10289062559604645\n",
      "Loss: nan Accuracy: 0.09617187827825546\n",
      "Loss: nan Accuracy: 0.10046874731779099\n",
      "Loss: nan Accuracy: 0.09734375029802322\n",
      "Loss: nan Accuracy: 0.09555137902498245\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(data2):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wLugV64_YoI"
   },
   "source": [
    "Source domain =Fashion  MNIST target domain= MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EmPXBuP4_WpO"
   },
   "outputs": [],
   "source": [
    "#Source domain =Fashion  MNIST target domain= MNIST\n",
    "train_steps = 1500\n",
    "\n",
    "layer_list = [ tf.keras.layers.Conv2D(filters = 64,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "              tf.keras.layers.Conv2D(filters = 64,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              tf.keras.layers.Dropout(0.5),\n",
    "              \n",
    "              tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              tf.keras.layers.Dropout(0.25),\n",
    "              \n",
    "               tf.keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "                    \n",
    "              \n",
    "              tf.keras.layers.Conv2D(filters=256,kernel_size=3,padding='same',activation=tf.nn.relu),\n",
    "               tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2),\n",
    "              \n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(7,activation='softmax')]  \n",
    "model = tf.keras.Sequential(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199404,
     "status": "ok",
     "timestamp": 1600127364015,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "23LWTO8k_fWG",
    "outputId": "13de12a5-39bb-447a-e27f-7c293b171e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan Accuracy: 0.1015625\n",
      "Loss: nan Accuracy: 0.09726562350988388\n",
      "Loss: nan Accuracy: 0.10257812589406967\n",
      "Loss: nan Accuracy: 0.09765625\n",
      "Loss: nan Accuracy: 0.09515625238418579\n",
      "Loss: nan Accuracy: 0.1005639061331749\n",
      "Loss: nan Accuracy: 0.09992187470197678\n",
      "Loss: nan Accuracy: 0.10046874731779099\n",
      "Loss: nan Accuracy: 0.10109374672174454\n",
      "Loss: nan Accuracy: 0.09632812440395355\n",
      "Loss: nan Accuracy: 0.09547305852174759\n",
      "Loss: nan Accuracy: 0.09742187708616257\n",
      "Loss: nan Accuracy: 0.09945312142372131\n",
      "Loss: nan Accuracy: 0.09648437798023224\n",
      "Loss: nan Accuracy: 0.10148437321186066\n",
      "Loss: nan Accuracy: 0.1011904776096344\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(data2):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 235309,
     "status": "ok",
     "timestamp": 1600127399922,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "9cU_3yGA_jRU",
    "outputId": "fcd018c9-45a9-406e-9946-378453c19e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan Accuracy: 0.09375\n",
      "Loss: nan Accuracy: 0.09749999642372131\n",
      "Loss: nan Accuracy: 0.09851562231779099\n",
      "Loss: nan Accuracy: 0.09828124940395355\n",
      "Loss: nan Accuracy: 0.1003125011920929\n",
      "Loss: nan Accuracy: 0.09750939905643463\n",
      "Loss: nan Accuracy: 0.09820312261581421\n",
      "Loss: nan Accuracy: 0.09703125059604645\n",
      "Loss: nan Accuracy: 0.10359375178813934\n",
      "Loss: nan Accuracy: 0.09929687529802322\n",
      "Loss: nan Accuracy: 0.0988408550620079\n",
      "Loss: nan Accuracy: 0.09656249731779099\n",
      "Loss: nan Accuracy: 0.09687499701976776\n",
      "Loss: nan Accuracy: 0.10164062678813934\n",
      "Loss: nan Accuracy: 0.09937500208616257\n",
      "Loss: nan Accuracy: 0.09656954556703568\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(data1):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iw3usir6CUUq"
   },
   "source": [
    "### Observation for 3.2\n",
    "\n",
    "\n",
    "Transfer learning performs worse in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCJLoDRx_k7J"
   },
   "source": [
    "## 3.3 Compare diffferent ways to do the transfer learning\n",
    "\n",
    "Source domain = MNIST target domain= Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2o4wpTQ_1s0"
   },
   "source": [
    "### Freezing all Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a10H9sLk_ygM"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 270809,
     "status": "ok",
     "timestamp": 1600127435426,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "ExjH34lBAE3Q",
    "outputId": "be11610a-337f-4e4e-8eff-682186da413f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan Accuracy: 0.140625\n",
      "Loss: nan Accuracy: 0.09468749910593033\n",
      "Loss: nan Accuracy: 0.09765625\n",
      "Loss: nan Accuracy: 0.10265625268220901\n",
      "Loss: nan Accuracy: 0.09726562350988388\n",
      "Loss: nan Accuracy: 0.09805764257907867\n",
      "Loss: nan Accuracy: 0.09828124940395355\n",
      "Loss: nan Accuracy: 0.09820312261581421\n",
      "Loss: nan Accuracy: 0.09914062172174454\n",
      "Loss: nan Accuracy: 0.09953124821186066\n",
      "Loss: nan Accuracy: 0.10087719559669495\n",
      "Loss: nan Accuracy: 0.09734375029802322\n",
      "Loss: nan Accuracy: 0.10187499970197678\n",
      "Loss: nan Accuracy: 0.099609375\n",
      "Loss: nan Accuracy: 0.09757812321186066\n",
      "Loss: nan Accuracy: 0.09656954556703568\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(data1):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmsH6J96AHOS"
   },
   "source": [
    "### Freezing the Top Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABxjCnb-ALNB"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[:-3]:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 305810,
     "status": "ok",
     "timestamp": 1600127470430,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "YJFDqrQ1AN1B",
    "outputId": "69f97212-2446-453f-8151-63aebdf5700e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan Accuracy: 0.0859375\n",
      "Loss: nan Accuracy: 0.09828124940395355\n",
      "Loss: nan Accuracy: 0.10429687798023224\n",
      "Loss: nan Accuracy: 0.09742187708616257\n",
      "Loss: nan Accuracy: 0.09234374761581421\n",
      "Loss: nan Accuracy: 0.09876253455877304\n",
      "Loss: nan Accuracy: 0.1003125011920929\n",
      "Loss: nan Accuracy: 0.10398437827825546\n",
      "Loss: nan Accuracy: 0.09875000268220901\n",
      "Loss: nan Accuracy: 0.09609375149011612\n",
      "Loss: nan Accuracy: 0.0958646610379219\n",
      "Loss: nan Accuracy: 0.10007812827825546\n",
      "Loss: nan Accuracy: 0.09882812201976776\n",
      "Loss: nan Accuracy: 0.09984374791383743\n",
      "Loss: nan Accuracy: 0.09664062410593033\n",
      "Loss: nan Accuracy: 0.09766604006290436\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(data1):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDUdVxx-AQk1"
   },
   "source": [
    "### Freezing the Bottom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-qLK7_sAU0b"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[-3:-1]:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 341167,
     "status": "ok",
     "timestamp": 1600127505791,
     "user": {
      "displayName": "Budhaditya Mukhopadhyay",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhEyQYtB08E19TTD1GE25cdVV6J1caJTjg47JiCTg=s64",
      "userId": "05521750824977221992"
     },
     "user_tz": -120
    },
    "id": "2CRur6L_AWyE",
    "outputId": "ced95eb3-4c14-44cd-d82d-6a662db38d87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan Accuracy: 0.078125\n",
      "Loss: nan Accuracy: 0.10117187350988388\n",
      "Loss: nan Accuracy: 0.10171875357627869\n",
      "Loss: nan Accuracy: 0.09882812201976776\n",
      "Loss: nan Accuracy: 0.09476562589406967\n",
      "Loss: nan Accuracy: 0.09602130204439163\n",
      "Loss: nan Accuracy: 0.09789062291383743\n",
      "Loss: nan Accuracy: 0.09726562350988388\n",
      "Loss: nan Accuracy: 0.09757812321186066\n",
      "Loss: nan Accuracy: 0.10273437201976776\n",
      "Loss: nan Accuracy: 0.10087719559669495\n",
      "Loss: nan Accuracy: 0.09687499701976776\n",
      "Loss: nan Accuracy: 0.10007812827825546\n",
      "Loss: nan Accuracy: 0.10187499970197678\n",
      "Loss: nan Accuracy: 0.09398437291383743\n",
      "Loss: nan Accuracy: 0.1005639061331749\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for step, (img_batch, lbl_batch) in enumerate(data1):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(img_batch)\n",
    "        # loss format is generally: first argument targets, second argument outputs\n",
    "        xent = loss_fn(lbl_batch, logits)\n",
    "\n",
    "    # if you didn't build the model, it is important that you get the variables\n",
    "    # AFTER the model has been called the first time\n",
    "    varis = model.trainable_variables\n",
    "    grads = tape.gradient(xent, varis)\n",
    "      \n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    train_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    if not step % 100:\n",
    "        # this is different from before. there, we only evaluated accuracy\n",
    "        # for one batch. Now, we always average over 100 batches\n",
    "        print(\"Loss: {} Accuracy: {}\".format(xent, train_acc_metric.result()))\n",
    "        train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ua_v5SLQCHeQ"
   },
   "source": [
    "### Observation for 3.3\n",
    "After freezing all the layers we can see that the accuracy has reached down to 9.65%\n",
    "\n",
    "Freezing the top layer gives us an accuracy of 9.7%\n",
    "\n",
    "Freezing the bottom layer gives us as accuracy of 10%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMAV1cyeRTtNiW9QUC+rsiU",
   "collapsed_sections": [
    "dXuNJQ5lxLgs"
   ],
   "mount_file_id": "1TzosA5s_H3CsK3AOBuaJqt6TjNIXB-Ir",
   "name": "TASK_3.ipynb",
   "provenance": [
    {
     "file_id": "1Uei5so0Cf8Bt95esXK-I2-Ak1vixJldJ",
     "timestamp": 1599420269634
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
