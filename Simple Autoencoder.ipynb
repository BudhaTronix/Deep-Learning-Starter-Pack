{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUQYJd0Q5WDe",
        "colab_type": "text"
      },
      "source": [

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_iSzCJW5N4k",
        "colab_type": "text"
      },
      "source": [
        "# **Simple Autoencoder**\n",
        "Inspired from: https://medium.com/red-buffer/autoencoders-guide-and-code-in-tensorflow-2-0-a4101571ce56"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKu97p-S8lg",
        "colab_type": "code",
        "outputId": "98dd4da3-be0e-4bdf-ddbe-5320a578e104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fashion = tf.keras.datasets.fashion_mnist\n",
        "(fx_train, fy_train), (fx_test, fy_test) = fashion.load_data()\n",
        "\n",
        "fx_train = tf.keras.utils.normalize(fx_train, axis=1)\n",
        "fx_test = tf.keras.utils.normalize(fx_test, axis=1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVlf-U3EhrPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyConnectedAutoEncoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(FullyConnectedAutoEncoder, self).__init__()\n",
        "        self.flatten_layer = tf.keras.layers.Flatten()\n",
        "        self.dense1 = tf.keras.layers.Dense(64)\n",
        "        #self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "        \n",
        "        \n",
        "        self.bottleneck = tf.keras.layers.Dense(32)\n",
        "    \n",
        "        #self.dense4 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "        self.dense5 = tf.keras.layers.Dense(64)\n",
        "        \n",
        "        self.dense_final = tf.keras.layers.Dense(784, activation=tf.nn.sigmoid)\n",
        "        \n",
        "    \n",
        "    def call(self, inp):\n",
        "        x_reshaped = self.flatten_layer(inp)\n",
        "        x = self.dense1(x_reshaped)\n",
        "        #x = self.dense2(x)\n",
        "        x = self.bottleneck(x)\n",
        "        #x = self.dense4(x)\n",
        "        x = self.dense5(x)\n",
        "        x = self.dense_final(x)\n",
        "        return x, x_reshaped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp6_bOlzhsNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(x, x_bar):\n",
        "    return tf.losses.binary_crossentropy(x, x_bar)\n",
        "def grad(model, inputs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        reconstruction, inputs_reshaped = model(inputs)\n",
        "        loss_value = loss(inputs_reshaped, reconstruction)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables), inputs_reshaped, reconstruction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raPgQzfehxso",
        "colab_type": "code",
        "outputId": "1684d93c-0389-4751-8921-5e9d5df99f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = FullyConnectedAutoEncoder()\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "global_step = tf.Variable(0)\n",
        "num_epochs = 10\n",
        "batch_size = 256\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    for x in range(0, len(fx_train), batch_size):\n",
        "        x_inp = fx_train[x : x + batch_size]\n",
        "        loss_value, grads, inputs_reshaped, reconstruction = grad(model, x_inp)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
        "                              global_step)\n",
        "        \n",
        "    if global_step.numpy() % 500 == 0:\n",
        "        print(\"Step: {}, Loss: {}\".format(global_step.numpy(),loss(inputs_reshaped, reconstruction).numpy()))\n",
        "    #print(\"Step: {},         Loss: {}\".format(global_step.numpy(),loss(inputs_reshaped, reconstruction).numpy()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "WARNING:tensorflow:Layer fully_connected_auto_encoder_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step: 0,         Loss: [0.35763106 0.41841173 0.28868386 0.30612436 0.314783   0.22597931\n",
            " 0.2149804  0.26130253 0.31176198 0.34140483 0.17535433 0.1905077\n",
            " 0.27242118 0.2802289  0.2004276  0.33633915 0.25545618 0.34702852\n",
            " 0.36112875 0.41943946 0.3268486  0.16526599 0.3472731  0.18544412\n",
            " 0.2586524  0.41524425 0.30972585 0.17377459 0.22992484 0.29935002\n",
            " 0.27780062 0.23095037 0.18407677 0.35462493 0.17987177 0.27361274\n",
            " 0.1884683  0.29220793 0.4000576  0.2953685  0.3280533  0.32655504\n",
            " 0.18654548 0.27229235 0.41771632 0.46211302 0.33828765 0.19044195\n",
            " 0.32257754 0.27378    0.21107744 0.21613254 0.25923145 0.35668612\n",
            " 0.31868005 0.34893098 0.2064443  0.3378592  0.35547566 0.34013414\n",
            " 0.35875815 0.34365073 0.18812059 0.3562917  0.34758407 0.30339578\n",
            " 0.2854458  0.21158282 0.23326598 0.35095137 0.3914148  0.3108926\n",
            " 0.30918583 0.33194903 0.3763694  0.24768366 0.24466869 0.4160632\n",
            " 0.3281381  0.19924265 0.30237707 0.31076643 0.33655462 0.32767323\n",
            " 0.33179244 0.17447998 0.2983423  0.15976924 0.22254618 0.28920376\n",
            " 0.3837415  0.24736771 0.16375466 0.21946545 0.3499788  0.1854559 ]\n",
            "Epoch:  1\n",
            "Step: 0,         Loss: [0.35401323 0.40459645 0.28439844 0.2851039  0.31019658 0.21857141\n",
            " 0.21018207 0.25726166 0.30644292 0.33178535 0.17292012 0.18823312\n",
            " 0.2643217  0.27444762 0.1974067  0.32945463 0.2504919  0.34330353\n",
            " 0.3522577  0.41388503 0.3209469  0.15980753 0.33651182 0.17441878\n",
            " 0.25472835 0.4027327  0.30663264 0.17008606 0.2203463  0.29652968\n",
            " 0.2742394  0.22846697 0.18064132 0.35066596 0.17711562 0.26858726\n",
            " 0.18524896 0.2700203  0.3954614  0.2942032  0.3245528  0.32174227\n",
            " 0.18465228 0.26417556 0.40188077 0.45746845 0.33646592 0.1733825\n",
            " 0.31902185 0.26963973 0.20616749 0.2161434  0.25723913 0.35071683\n",
            " 0.31210458 0.3452059  0.20418763 0.3371412  0.33846167 0.3379499\n",
            " 0.35443506 0.3402776  0.18488577 0.3488703  0.34380174 0.3009009\n",
            " 0.27699593 0.21045518 0.22831567 0.3476523  0.3804933  0.30759928\n",
            " 0.3063122  0.3300074  0.3347622  0.24187559 0.24361396 0.40735012\n",
            " 0.32635546 0.19622391 0.2956928  0.30682793 0.33426604 0.3246743\n",
            " 0.32974425 0.16975453 0.29484746 0.1531389  0.21940371 0.28594485\n",
            " 0.3713459  0.23931    0.15711865 0.21808632 0.341135   0.1734997 ]\n",
            "Epoch:  2\n",
            "Step: 0,         Loss: [0.3512774  0.40338224 0.28259462 0.27795497 0.30913392 0.21582012\n",
            " 0.20351723 0.25531298 0.30376977 0.32040715 0.17148954 0.18753886\n",
            " 0.26172736 0.27157512 0.19721358 0.3245703  0.2486939  0.34186584\n",
            " 0.34967935 0.40727937 0.30136895 0.15796438 0.32884264 0.1682645\n",
            " 0.25390112 0.39569154 0.30550614 0.16863878 0.21744841 0.29553738\n",
            " 0.27191034 0.22508134 0.17971444 0.3200244  0.17568612 0.2698526\n",
            " 0.1830487  0.26435152 0.3959947  0.29310757 0.32404873 0.32063642\n",
            " 0.18396103 0.26139033 0.39865267 0.45666203 0.33496055 0.16795687\n",
            " 0.31817296 0.2671153  0.20397167 0.21404654 0.25643483 0.34665883\n",
            " 0.3095084  0.3438639  0.2012175  0.336385   0.33423102 0.3375487\n",
            " 0.3514109  0.33572316 0.18401882 0.34486577 0.34270656 0.29798815\n",
            " 0.27395272 0.20936137 0.22469205 0.34549522 0.3789289  0.3065785\n",
            " 0.30472565 0.3294188  0.32451257 0.24094743 0.24071237 0.40435615\n",
            " 0.3256343  0.1932739  0.29319188 0.30574003 0.33373478 0.32239753\n",
            " 0.32832018 0.1682197  0.29351082 0.15054944 0.21747826 0.2838475\n",
            " 0.36414212 0.23562697 0.15530077 0.21652845 0.33671808 0.1643732 ]\n",
            "Epoch:  3\n",
            "Step: 0,         Loss: [0.34710157 0.4001532  0.28219056 0.27425197 0.3073438  0.213935\n",
            " 0.19853047 0.25341347 0.29927772 0.31557098 0.17055364 0.18692239\n",
            " 0.26048177 0.26961225 0.19687988 0.32356614 0.24376905 0.3390579\n",
            " 0.34668583 0.4051932  0.27886832 0.15681218 0.32369053 0.16693631\n",
            " 0.25394356 0.3937588  0.30479792 0.1678152  0.2152172  0.29486585\n",
            " 0.27035633 0.22212173 0.17906201 0.29916087 0.17465802 0.2678686\n",
            " 0.18189639 0.26201007 0.39620638 0.29219812 0.32304543 0.31950858\n",
            " 0.1831969  0.25956926 0.39431062 0.45147386 0.33367857 0.16574845\n",
            " 0.317198   0.2645241  0.20093808 0.21267545 0.25519744 0.34508032\n",
            " 0.30521148 0.342219   0.19718665 0.33532122 0.3296345  0.3370738\n",
            " 0.34863484 0.3291334  0.18289278 0.34385976 0.34199524 0.294238\n",
            " 0.2733612  0.20685679 0.22003593 0.34295344 0.37537128 0.30620465\n",
            " 0.30422792 0.32765985 0.3195366  0.2385222  0.23733856 0.40214348\n",
            " 0.32493073 0.19089362 0.291929   0.30502412 0.33320564 0.32068065\n",
            " 0.3268751  0.16730875 0.29298005 0.14949244 0.21559536 0.2824697\n",
            " 0.36197293 0.22374491 0.1542812  0.21446426 0.33324558 0.16087817]\n",
            "Epoch:  4\n",
            "Step: 0,         Loss: [0.34487346 0.3965841  0.28161833 0.2707563  0.30588737 0.21316808\n",
            " 0.19737059 0.2523935  0.2975543  0.31284907 0.17027989 0.18645583\n",
            " 0.25954413 0.26823136 0.19638322 0.32331455 0.24218252 0.33808038\n",
            " 0.34453416 0.40390208 0.266126   0.15598418 0.3213322  0.16585536\n",
            " 0.25413388 0.39294544 0.30423942 0.16754876 0.21377595 0.2944685\n",
            " 0.268843   0.22089823 0.17875037 0.2926544  0.1740181  0.26625648\n",
            " 0.18100482 0.26000062 0.39570603 0.29115796 0.32230663 0.31856322\n",
            " 0.18274078 0.25844592 0.39183822 0.4461613  0.33299014 0.16505148\n",
            " 0.31657064 0.26360336 0.19916409 0.2106433  0.2538329  0.34441072\n",
            " 0.30368733 0.341463   0.19499016 0.33451894 0.32723475 0.33682212\n",
            " 0.34699374 0.32650748 0.1821361  0.34328538 0.34123978 0.29273248\n",
            " 0.2710724  0.20523763 0.21449636 0.3417275  0.3731996  0.30606923\n",
            " 0.30407584 0.32644105 0.31729963 0.23732406 0.23512317 0.39876744\n",
            " 0.3237706  0.18948188 0.29150984 0.30416623 0.3324089  0.32004258\n",
            " 0.326242   0.16697    0.29258278 0.14908478 0.21434562 0.28186908\n",
            " 0.36146188 0.21636173 0.15394208 0.21252751 0.33134648 0.15837516]\n",
            "Epoch:  5\n",
            "Step: 0,         Loss: [0.34355783 0.39399326 0.28146422 0.26803708 0.30438867 0.21291047\n",
            " 0.19664963 0.2511541  0.29703996 0.31243762 0.1697429  0.18571433\n",
            " 0.25878847 0.26744533 0.19557545 0.32314968 0.24104655 0.33771288\n",
            " 0.34329575 0.4035798  0.2625401  0.15544276 0.3201876  0.16455187\n",
            " 0.2535891  0.39259946 0.30365917 0.16713363 0.213063   0.29437175\n",
            " 0.26816154 0.22023536 0.17834313 0.2914605  0.17306748 0.26428467\n",
            " 0.18064065 0.25683805 0.39585    0.29071727 0.32203925 0.31811526\n",
            " 0.18197767 0.25782102 0.38974568 0.44236904 0.33248752 0.16463038\n",
            " 0.31646654 0.26373804 0.19777195 0.20963916 0.25183153 0.34417385\n",
            " 0.3027025  0.341132   0.19394933 0.3340706  0.32701084 0.3373923\n",
            " 0.34586355 0.32559434 0.18125542 0.34295312 0.3408306  0.29130673\n",
            " 0.26818705 0.20358557 0.21256937 0.34117427 0.37203366 0.3061326\n",
            " 0.30417183 0.32557267 0.31748718 0.23643276 0.23412946 0.39443183\n",
            " 0.32312813 0.18916406 0.2918162  0.30326077 0.3318002  0.319696\n",
            " 0.3259511  0.16655932 0.29250464 0.14873873 0.21282706 0.28153178\n",
            " 0.36120126 0.21321253 0.15352395 0.21101785 0.33043545 0.1563123 ]\n",
            "Epoch:  6\n",
            "Step: 0,         Loss: [0.342425   0.39240727 0.28073493 0.26627994 0.30355313 0.21263878\n",
            " 0.19629963 0.25040528 0.2964097  0.3117478  0.16951662 0.18525104\n",
            " 0.2584134  0.26664343 0.19498686 0.3221397  0.24048282 0.337691\n",
            " 0.34181973 0.40159896 0.26020622 0.15482517 0.3197814  0.16379532\n",
            " 0.25348836 0.39193228 0.30326837 0.16686337 0.21255898 0.2938461\n",
            " 0.26765266 0.2199696  0.17805575 0.2905506  0.17244121 0.2638352\n",
            " 0.18009442 0.25506946 0.3958618  0.2901911  0.32140642 0.31740484\n",
            " 0.18155168 0.25758353 0.39044225 0.4423153  0.3320948  0.16419314\n",
            " 0.3158924  0.26313022 0.19671018 0.20868824 0.25045452 0.3433317\n",
            " 0.3021968  0.3406877  0.19268079 0.33268258 0.3250748  0.3370429\n",
            " 0.34552774 0.3250693  0.18050085 0.34250018 0.34023765 0.2897386\n",
            " 0.26647297 0.20308064 0.21102217 0.34093818 0.37295127 0.3057046\n",
            " 0.30367216 0.32491145 0.316323   0.23562202 0.23321235 0.3919871\n",
            " 0.32222712 0.18893604 0.29144412 0.30254963 0.3313971  0.31909877\n",
            " 0.3251699  0.16606109 0.29183605 0.148404   0.21155794 0.28064787\n",
            " 0.36115727 0.21182181 0.15310122 0.21026339 0.32930866 0.15443559]\n",
            "Epoch:  7\n",
            "Step: 0,         Loss: [0.34108657 0.391331   0.2804482  0.26490924 0.30317786 0.212549\n",
            " 0.19575302 0.24987406 0.29620954 0.31154975 0.16933435 0.18466604\n",
            " 0.25787592 0.26607156 0.19424406 0.3220351  0.24013293 0.3374236\n",
            " 0.34113583 0.40158173 0.2589401  0.15417907 0.31973457 0.16353436\n",
            " 0.25291216 0.39149186 0.3029552  0.1666245  0.21246178 0.29374927\n",
            " 0.26726553 0.21970432 0.17766596 0.28999045 0.17181392 0.263302\n",
            " 0.17983633 0.2537216  0.39623907 0.28987184 0.3211733  0.31708196\n",
            " 0.18103783 0.2572239  0.3896929  0.43998057 0.3316905  0.16403644\n",
            " 0.31576976 0.2630546  0.19588758 0.20806044 0.24912769 0.3429977\n",
            " 0.301916   0.34024608 0.19181152 0.3324806  0.32472152 0.3371442\n",
            " 0.34514025 0.32477173 0.17981993 0.34241235 0.33987185 0.28815114\n",
            " 0.2653534  0.20264201 0.21025684 0.34077173 0.37357914 0.30554068\n",
            " 0.30352503 0.3241603  0.31629568 0.2352083  0.232663   0.39070755\n",
            " 0.32200217 0.18885693 0.2915529  0.3016494  0.3310158  0.31858304\n",
            " 0.3246573  0.16574351 0.29155087 0.14803593 0.21064904 0.28032553\n",
            " 0.36149016 0.21001478 0.15260756 0.2097322  0.32872266 0.1531285 ]\n",
            "Epoch:  8\n",
            "Step: 0,         Loss: [0.33965853 0.39049965 0.28008908 0.26374692 0.3030728  0.2123469\n",
            " 0.19508387 0.24954313 0.2959283  0.31118113 0.16927613 0.18417814\n",
            " 0.2573326  0.26551074 0.193394   0.3217428  0.23985769 0.3371587\n",
            " 0.34006843 0.40132883 0.25760803 0.15352601 0.31960282 0.16343366\n",
            " 0.25244445 0.3909637  0.302645   0.16646709 0.21229266 0.29353362\n",
            " 0.26694608 0.21935311 0.17732866 0.289382   0.17120591 0.26289392\n",
            " 0.17964187 0.25317565 0.39652634 0.28946963 0.32091197 0.31680816\n",
            " 0.18054824 0.25680196 0.389513   0.4389671  0.3313377  0.16400154\n",
            " 0.31559905 0.26270932 0.19546789 0.20762706 0.24818681 0.34266663\n",
            " 0.3017147  0.3398589  0.1912003  0.33226296 0.32427004 0.337\n",
            " 0.34486303 0.3244974  0.17942071 0.3422053  0.33953017 0.2867529\n",
            " 0.265029   0.20240282 0.20974661 0.34063694 0.37329775 0.3053337\n",
            " 0.30334246 0.32346645 0.31589967 0.23475488 0.23210709 0.38991657\n",
            " 0.32179847 0.18870805 0.2913988  0.3012364  0.3307448  0.31816095\n",
            " 0.32418272 0.16550407 0.2911393  0.14766821 0.20980042 0.2799924\n",
            " 0.36172384 0.20756608 0.15225805 0.20929389 0.3276572  0.15251198]\n",
            "Epoch:  9\n",
            "Step: 0,         Loss: [0.33845127 0.38986346 0.27979484 0.26289338 0.30301428 0.21212526\n",
            " 0.19444753 0.2493258  0.29573315 0.31079003 0.16919191 0.18376425\n",
            " 0.2567927  0.2650817  0.19253889 0.32166746 0.23952737 0.3369062\n",
            " 0.33897853 0.40161896 0.2562342  0.15286857 0.31911457 0.16331153\n",
            " 0.2518978  0.3906307  0.30238727 0.16632392 0.21211836 0.29342076\n",
            " 0.26661864 0.21876934 0.17699997 0.28877    0.17059758 0.26243243\n",
            " 0.17950192 0.25236496 0.3966601  0.28905496 0.32070506 0.31658784\n",
            " 0.18007706 0.25632936 0.38846144 0.43810156 0.3310364  0.16390218\n",
            " 0.31548485 0.26218766 0.19510986 0.20734219 0.24754989 0.34248453\n",
            " 0.3015486  0.33960778 0.1906931  0.33224946 0.32408184 0.33694738\n",
            " 0.3445166  0.324249   0.17902324 0.3420355  0.33938485 0.28573537\n",
            " 0.26506007 0.20212504 0.20946658 0.3405445  0.3720407  0.30518058\n",
            " 0.3032542  0.3228432  0.31575993 0.23427165 0.23154919 0.38952944\n",
            " 0.32172915 0.18850331 0.29129145 0.30104282 0.33060235 0.31793213\n",
            " 0.32388067 0.16532727 0.29077005 0.1473304  0.20904452 0.27974793\n",
            " 0.3618603  0.20448098 0.15202644 0.20885287 0.3261225  0.15233044]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH1eP06ozlup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(fx_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP0N5Lex2dYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "array = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "#print(array[np.argmax(predictions[67])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwFQh14hzmps",
        "colab_type": "code",
        "outputId": "89c117ee-f94a-4bd9-f6a3-cc0d9c302077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(fx_test[67], cmap=plt.cm.binary)\n",
        "print(array[fy_test[67]])\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dress\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAPcElEQVR4nO3dX2xd1ZXH8d/Ccf46goRYxkqjSVMQEhogra7CoEYVo2oqQEKhLyh5qDIITfoAUir1oYg+lEc0mrbqA6qUDlHSUYeqqA3kAU3KRBWoQpQ4KEMSEAQiQxJM4vwjzh9iO1nz4ENlB9+9nXvO/YPX9yNZvr7rHp/lk/x8ru+++2xzdwGY/W5odwMAWoOwA0EQdiAIwg4EQdiBIOa0cmfLli3zlStXtnKXIVy9erVu7fz588lt58+fn6znRmtGR0eT9Z6enro1M0tui+s3ODiokydPTntgS4XdzO6X9CtJXZL+092fST1+5cqVGhgYKLNLTOPChQt1a6+//npy29tvvz1ZHxsbS9YHBweT9bVr19atzZs3L7ktrl+tVqtba/hpvJl1SXpW0gOS7pC0wczuaPT7AWiuMn+zr5H0gbsfdvdRSb+XtK6atgBUrUzYl0s6Munro8V9U5jZJjMbMLOB4eHhErsDUEbTX4139y3uXnP3Wm9vb7N3B6COMmE/JmnFpK+/VtwHoAOVCfseSbeZ2dfNbK6k9ZJ2VtMWgKo1PPTm7uNm9oSkXZoYetvq7gcr6+wrJDXOLUk33JD+nTo+Pp6sv/HGG8n6xx9/XLd27ty55Lbvvfdesp7rfcGCBcn6Cy+8ULeWe8/FPffck6x3d3cn66l/l9zPNRuVGmd395clvVxRLwCaKN6vNyAowg4EQdiBIAg7EARhB4Ig7EAQLZ3PPluVHbN99tlnk/XUnPBcfenSpcltc+PwubHshQsXJuup+fCHDh1Kbrt3795kffPmzcl6xLH0FI4GEARhB4Ig7EAQhB0IgrADQRB2IAiG3lrg8OHDyfrZs2eT9f7+/mS9q6urbi03/TZ3Kenc0Fpq31J6+u6iRYuS26am7kr547pq1apkPRrO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsLZAbR79y5Uqpemoq55w56X/isvvOLbucmuKaew9Abt+544qpOLMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs7fAyZMnk/WxsbFkPTfenBpLz21bdrnp1Dh6rp7bd24p61OnTiXrmKpU2M1sUNKIpCuSxt29VkVTAKpXxZn9n909feoC0Hb8zQ4EUTbsLunPZrbXzDZN9wAz22RmA2Y2MDw8XHJ3ABpVNuxr3f1bkh6Q9LiZfefaB7j7FnevuXutt7e35O4ANKpU2N39WPH5hKQdktZU0RSA6jUcdjNbZGaLv7gt6XuSDlTVGIBqlXk1vk/SjmI+8xxJ/+3u/1NJV7PM6dOnk/XcWHWZ8ejc984pO06fque+d6733PsXMFXDYXf3w5LurrAXAE3E0BsQBGEHgiDsQBCEHQiCsANBMMW1BXJvE85djjk3vJXbPqXMZarL1nNTe3NDb7z9+vpwZgeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnb4HcJY9z48m5sfCUMksqz2TfuXH2lNz7B3JyU4cxFWd2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYWGBkZSdZ7enqS9RtvvDFZv3TpUt1ad3d3ctuy4/C57UdHR+vWcj/XvHnzkvVz584l65iKMzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4ewUOHEgvS59aUlmSFi9enKzfeuutyfqbb75ZtzZ37tzktjm5Oee5+ewXL16sW7vrrruS23744YfJ+tmzZ5P1/fv3163deeedyW1no+yZ3cy2mtkJMzsw6b6lZvaKmR0qPi9pbpsAyprJ0/htku6/5r4nJe1299sk7S6+BtDBsmF399ckXXv9n3WSthe3t0t6uOK+AFSs0Rfo+tx9qLj9qaS+eg80s01mNmBmA6zNBbRP6VfjfWKmRN3ZEu6+xd1r7l7r7e0tuzsADWo07MfNrF+Sis8nqmsJQDM0GvadkjYWtzdKeqmadgA0S3ac3cyel3SfpGVmdlTSzyQ9I+kPZvaYpI8kPdLMJjvdiy++WGr73LXZc/O2u7q66tZy89HLKjMO/9lnnyW3Ldv7rl276tYijrNnw+7uG+qUvltxLwCaiLfLAkEQdiAIwg4EQdiBIAg7EARTXCvw/vvvJ+tLlqQnBeaGr3JLE8+ZU/+fMTe9Nid3qehc76lLWZ85c6bU987Zs2dPqe1nG87sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wVOH/+fLKeG2e/5ZZbkvXcWHeuXkZummkzp9DefPPNyfrQ0FCyXvY9BrMNZ3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9grMnz+/1Pa5JZtn63hx7ufKjbPn9PT0lNp+tuHMDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM5egUuXLiXrixYtStYvXLiQrOfG8VPz2XPzzcvOlS+zZPPY2Fhy28uXLyfrueN65MiRZD2a7JndzLaa2QkzOzDpvqfN7JiZ7Ss+HmxumwDKmsnT+G2S7p/m/l+6++ri4+Vq2wJQtWzY3f01Sen1hwB0vDIv0D1hZm8XT/PrXmTNzDaZ2YCZDQwPD5fYHYAyGg37ryV9Q9JqSUOSfl7vge6+xd1r7l7r7e1tcHcAymoo7O5+3N2vuPtVSb+RtKbatgBUraGwm1n/pC+/L+lAvccC6AzZcXYze17SfZKWmdlRST+TdJ+ZrZbkkgYl/bCJPXa8Xbt2JeurVq1K1k+dOpWs9/f3J+tllL3ue9lx/JTcuvS5cfodO3Y0vO/ZKBt2d98wzd3PNaEXAE3E22WBIAg7EARhB4Ig7EAQhB0IgimuFbj33ntLbZ9bsrnMpaSbOTRW9vuPjo4mt80NOR49ejRZf+ihh5L1aDizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLNXYPfu3aW2P3PmTLK+bdu2ZL2vr69urexyz7lx9Fw9dRnsTz75JLnto48+mqyvX78+WcdUnNmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2StQZtliKX9J5CtXrjRcLztfPbd9rrfU9rnjlpvvjuvDmR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcvQJlx9m7urpK7b/Mssu53nI/W66eew9BSq63nNRc/jlz4v3Xzx5NM1thZn8xs3fM7KCZbS7uX2pmr5jZoeLzkua3C6BRM/nVOS7px+5+h6R/kvS4md0h6UlJu939Nkm7i68BdKhs2N19yN3fKm6PSHpX0nJJ6yRtLx62XdLDzWoSQHnX9UeRma2U9E1Jf5PU5+5DRelTSdNeCM3MNpnZgJkNDA8Pl2gVQBkzDruZ9Uj6o6Qfufu5yTWfeIVo2leJ3H2Lu9fcvdbb21uqWQCNm1HYzaxbE0H/nbv/qbj7uJn1F/V+SSea0yKAKmTHH2xijuJzkt51919MKu2UtFHSM8Xnl5rS4VdA2SGi3PY33XRTsp4a/irbW05u6C01LJj7uZrdezQzGWz8tqQfSNpvZvuK+57SRMj/YGaPSfpI0iPNaRFAFbJhd/e/Sqp3BYLvVtsOgGbheRIQBGEHgiDsQBCEHQiCsANBxJvn1wS5Sx6nli2Wyl/uucwU15wy4+gz2b6ZUv8uTHEFMGsRdiAIwg4EQdiBIAg7EARhB4Ig7EAQ8QYbO1A7LyXd7HHydo6zYyrO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsHaDs3OrUWHluHD237+7u7oZ6+kJqnL2T58LPRpzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCImazPvkLSbyX1SXJJW9z9V2b2tKR/kzRcPPQpd3+5WY3OZnPnzk3Wy1xXPjeWffHixWT9888/T9Zz4/Spufq5befNm5es57C++1QzeTfHuKQfu/tbZrZY0l4ze6Wo/dLd/6N57QGoykzWZx+SNFTcHjGzdyUtb3ZjAKp1Xc9zzGylpG9K+ltx1xNm9raZbTWzJXW22WRmA2Y2MDw8PN1DALTAjMNuZj2S/ijpR+5+TtKvJX1D0mpNnPl/Pt127r7F3WvuXuvt7a2gZQCNmFHYzaxbE0H/nbv/SZLc/bi7X3H3q5J+I2lN89oEUFY27DbxUvBzkt51919Mur9/0sO+L+lA9e0BqMpMXo3/tqQfSNpvZvuK+56StMHMVmtiOG5Q0g+b0mEAly9fTtYvXbqUrKeGqEZGRpLbLl+efq119erVyfrBgweT9VTv4+PjyW3LDr1hqpm8Gv9XSdMN9DKmDnyF8K4DIAjCDgRB2IEgCDsQBGEHgiDsQBBcSroCZS8FvXDhwmS9Vqsl63fffXfd2oIFCxrqaaZyvaWm0L766qvJbcsuZc0U16k4GkAQhB0IgrADQRB2IAjCDgRB2IEgCDsQhOUuNVzpzsyGJX006a5lkk62rIHr06m9dWpfEr01qsre/sHdp73+W0vD/qWdmw24e/pdGW3Sqb11al8SvTWqVb3xNB4IgrADQbQ77FvavP+UTu2tU/uS6K1RLemtrX+zA2iddp/ZAbQIYQeCaEvYzex+M3vPzD4wsyfb0UM9ZjZoZvvNbJ+ZDbS5l61mdsLMDky6b6mZvWJmh4rP066x16benjazY8Wx22dmD7aptxVm9hcze8fMDprZ5uL+th67RF8tOW4t/5vdzLokvS/pXyQdlbRH0gZ3f6eljdRhZoOSau7e9jdgmNl3JJ2X9Ft3/8fivn+XdNrdnyl+US5x9590SG9PSzrf7mW8i9WK+icvMy7pYUn/qjYeu0Rfj6gFx60dZ/Y1kj5w98PuPirp95LWtaGPjufur0k6fc3d6yRtL25v18R/lpar01tHcPchd3+ruD0i6Ytlxtt67BJ9tUQ7wr5c0pFJXx9VZ6337pL+bGZ7zWxTu5uZRp+7DxW3P5XU185mppFdxruVrllmvGOOXSPLn5fFC3RfttbdvyXpAUmPF09XO5JP/A3WSWOnM1rGu1WmWWb879p57Bpd/rysdoT9mKQVk77+WnFfR3D3Y8XnE5J2qPOWoj7+xQq6xecTbe7n7zppGe/plhlXBxy7di5/3o6w75F0m5l93czmSlovaWcb+vgSM1tUvHAiM1sk6XvqvKWod0raWNzeKOmlNvYyRacs411vmXG1+di1fflzd2/5h6QHNfGK/IeSftqOHur0tUrS/xUfB9vdm6TnNfG0bkwTr208JulmSbslHZL0v5KWdlBv/yVpv6S3NRGs/jb1tlYTT9HflrSv+Hiw3ccu0VdLjhtvlwWC4AU6IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQji/wGcX1MD8LHfCAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sryEN50x5wwi",
        "colab_type": "text"
      },
      "source": [
        "# **Convolutional Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAgM6uL551Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fx_train = tf.reshape(fx_train, (len(fx_train), 28, 28, 1))\n",
        "fx_test = tf.reshape(fx_test, (len(fx_test), 28, 28, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkw-GgZP7Xod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNetAutoEncoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ConvNetAutoEncoder, self).__init__()\n",
        "        \n",
        "        self.conv1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')\n",
        "        self.maxp1 = tf.keras.layers.MaxPooling2D((2, 2), padding='same')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')\n",
        "        self.maxp2 = tf.keras.layers.MaxPooling2D((2, 2), padding='same')\n",
        "        self.conv3 = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')\n",
        "        \n",
        "        self.encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')\n",
        "        \n",
        "        self.conv4 = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')\n",
        "        self.upsample1 = tf.keras.layers.UpSampling2D((2, 2))\n",
        "        self.conv5 = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')\n",
        "        self.upsample2 = tf.keras.layers.UpSampling2D((2, 2))\n",
        "        self.conv6 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')\n",
        "        self.upsample3 = tf.keras.layers.UpSampling2D((2, 2))\n",
        "        self.conv7 = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n",
        "        \n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxp1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxp2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.encoded(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.upsample1(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.upsample2(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.upsample3(x)\n",
        "        x = self.conv7(x)\n",
        "        return x\n",
        "def loss(x, x_bar):\n",
        "    return tf.losses.mean_squared_error(x, x_bar)\n",
        "def grad(model, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        reconstruction = model(inputs)\n",
        "        loss_value = loss(targets, reconstruction)\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables), reconstruction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v86f-ckg7epZ",
        "colab_type": "code",
        "outputId": "78b76db8-c987-493a-afaa-8e70d4a00f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = ConvNetAutoEncoder()\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "global_step = tf.Variable(0)\n",
        "num_epochs = 5\n",
        "batch_size = 4\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    for x in range(0, len(fx_train), batch_size):\n",
        "        x_inp = fx_train[x : x + batch_size]\n",
        "        loss_value, grads, reconstruction = grad(model, x_inp, x_inp)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables),\n",
        "                              global_step)\n",
        "        \n",
        "    if global_step.numpy() % 100000 == 0:\n",
        "        print(\"Step: {},         Loss: {}\".format(global_step.numpy(),\n",
        "                                      loss(x_inp, reconstruction).numpy()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "WARNING:tensorflow:Layer conv_net_auto_encoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Step: 0,         Loss: [[[9.66248696e-12 3.18683017e-16 1.14137782e-16 ... 1.42316848e-16\n",
            "   3.39789331e-16 8.08777142e-11]\n",
            "  [5.47184078e-16 2.93431392e-22 7.74265969e-23 ... 8.96798029e-23\n",
            "   2.74353504e-22 1.42863643e-14]\n",
            "  [7.40092589e-17 1.76491030e-23 4.15552124e-24 ... 4.76532847e-24\n",
            "   1.57088791e-23 2.08371016e-15]\n",
            "  ...\n",
            "  [1.42544180e-13 6.57172404e-19 1.18657611e-19 ... 7.84035128e-24\n",
            "   3.91235503e-23 1.46146726e-14]\n",
            "  [4.07092681e-13 2.68756234e-18 5.01528187e-19 ... 6.75231361e-23\n",
            "   3.23263713e-22 6.21316421e-14]\n",
            "  [1.07434621e-08 4.37490669e-12 1.37474125e-12 ... 8.33942900e-15\n",
            "   2.42525658e-14 3.30470162e-09]]\n",
            "\n",
            " [[1.71302070e-10 1.39803584e-14 4.77889340e-15 ... 8.18008635e-17\n",
            "   1.70397909e-16 4.71523445e-11]\n",
            "  [3.32441153e-14 6.52500941e-20 1.65642061e-20 ... 4.22566786e-23\n",
            "   1.07374054e-22 6.91877391e-15]\n",
            "  [6.24282256e-15 6.18116852e-21 1.44686753e-21 ... 2.21422449e-24\n",
            "   5.95096551e-24 9.80134646e-16]\n",
            "  ...\n",
            "  [9.05719413e-16 4.69549903e-22 6.37358023e-23 ... 1.27095337e-26\n",
            "   7.46571243e-26 3.00971188e-16]\n",
            "  [3.07274140e-15 2.45454760e-21 3.50045099e-22 ... 1.32427806e-25\n",
            "   7.48665040e-25 1.46985871e-15]\n",
            "  [4.78983797e-10 5.15573979e-14 1.34680847e-14 ... 1.60136952e-16\n",
            "   5.20863482e-16 2.97869895e-10]]\n",
            "\n",
            " [[2.78952902e-12 3.40479418e-17 9.76770850e-18 ... 2.81420771e-20\n",
            "   4.50354639e-20 1.64701640e-13]\n",
            "  [1.23182360e-16 1.81958057e-23 3.66104232e-24 ... 7.28834098e-28\n",
            "   1.31055700e-27 3.29184052e-18]\n",
            "  [1.81104065e-17 1.24153197e-24 2.28480644e-25 ... 2.36367757e-29\n",
            "   4.53069964e-29 3.38458939e-19]\n",
            "  ...\n",
            "  [5.92010253e-17 8.94576865e-24 9.74846697e-25 ... 3.78523228e-32\n",
            "   1.64431782e-31 6.74815236e-20]\n",
            "  [2.35013473e-16 5.75765404e-23 6.58731949e-24 ... 1.01341023e-30\n",
            "   4.21031909e-30 6.28335492e-19]\n",
            "  [9.38906800e-11 4.88753935e-15 1.09916946e-15 ... 1.00149070e-19\n",
            "   2.62742780e-19 2.12637590e-12]]\n",
            "\n",
            " [[4.45785661e-13 3.12564091e-18 1.10369303e-18 ... 1.49712180e-17\n",
            "   2.00717956e-17 8.60550867e-12]\n",
            "  [1.70389379e-17 1.67047903e-24 4.52304148e-25 ... 3.64713155e-24\n",
            "   5.20708427e-24 5.96778969e-16]\n",
            "  [4.20313878e-18 2.43212695e-25 6.25301320e-26 ... 1.60319585e-25\n",
            "   2.32646889e-25 7.21213019e-17]\n",
            "  ...\n",
            "  [8.44933893e-27 1.29222229e-37 4.86452774e-39 ... 1.54843480e-42\n",
            "   1.40045769e-41 3.66679743e-26]\n",
            "  [6.65271793e-26 2.05590385e-36 8.24324472e-38 ... 5.55306555e-41\n",
            "   4.80178741e-40 4.20373277e-25]\n",
            "  [6.97209958e-17 1.09208515e-23 1.20478091e-24 ... 2.32971826e-26\n",
            "   1.01275960e-25 1.99985154e-16]]]\n",
            "Epoch:  1\n",
            "Step: 0,         Loss: [[[3.05898565e-24 4.44765685e-32 1.21405850e-32 ... 3.53774060e-24\n",
            "   6.50166457e-24 3.71783116e-16]\n",
            "  [8.16290369e-34 2.87266185e-43 5.04467447e-44 ... 1.32628059e-32\n",
            "   2.89030610e-32 8.83009972e-21]\n",
            "  [1.18731365e-35 1.40129846e-45 0.00000000e+00 ... 2.24190374e-34\n",
            "   4.97566509e-34 5.99405445e-22]\n",
            "  ...\n",
            "  [5.43927009e-24 2.10288303e-30 3.77601864e-31 ... 3.42777686e-28\n",
            "   9.82101205e-28 3.14330320e-17]\n",
            "  [7.20347515e-23 5.05254901e-29 8.90488316e-30 ... 8.43610317e-27\n",
            "   2.37513292e-26 2.60653743e-16]\n",
            "  [1.92360589e-15 1.46143086e-18 4.13604756e-19 ... 7.36793859e-17\n",
            "   1.52167365e-16 4.37197445e-10]]\n",
            "\n",
            " [[1.05740057e-21 6.99905271e-29 1.57555830e-29 ... 2.18710937e-24\n",
            "   3.34859661e-24 2.15230675e-16]\n",
            "  [3.31820802e-30 6.82890997e-39 9.56684678e-40 ... 7.06939511e-33\n",
            "   1.19749990e-32 4.45593512e-21]\n",
            "  [7.64387183e-32 5.61500295e-41 6.89719104e-42 ... 1.18826132e-34\n",
            "   2.01544912e-34 2.98034048e-22]\n",
            "  ...\n",
            "  [1.12375335e-27 4.16054229e-35 6.62379157e-36 ... 5.33813755e-31\n",
            "   1.74093215e-30 6.64120411e-19]\n",
            "  [2.39383462e-26 1.88935021e-33 3.08903683e-34 ... 1.89974663e-29\n",
            "   6.08575566e-29 7.06273608e-18]\n",
            "  [1.01409357e-17 2.89686769e-21 7.86516183e-22 ... 1.86076078e-18\n",
            "   4.18656043e-18 5.21621323e-11]]\n",
            "\n",
            " [[9.75461825e-22 2.82601812e-29 4.75460043e-30 ... 7.71957740e-28\n",
            "   9.28923583e-28 7.54585226e-19]\n",
            "  [4.40991953e-30 3.38421987e-39 3.32348759e-40 ... 2.03059492e-37\n",
            "   2.47638496e-37 3.49982045e-24]\n",
            "  [1.45268944e-31 4.56234754e-41 4.21230318e-42 ... 2.88237986e-39\n",
            "   3.55286053e-39 2.12727880e-25]\n",
            "  ...\n",
            "  [3.67994121e-26 2.93001371e-33 6.29530306e-34 ... 1.60497438e-35\n",
            "   4.60698707e-35 9.17753961e-22]\n",
            "  [6.01842396e-25 9.93448479e-32 2.24017926e-32 ... 1.19907642e-33\n",
            "   3.33763915e-33 1.59154316e-20]\n",
            "  [8.54888384e-17 3.94997096e-20 1.34855231e-20 ... 4.91262889e-21\n",
            "   9.91284845e-21 1.35828967e-12]]\n",
            "\n",
            " [[1.43845850e-21 5.70435954e-29 1.21709760e-29 ... 1.05402775e-25\n",
            "   9.57085493e-26 1.46496482e-17]\n",
            "  [8.58539696e-30 1.13133874e-38 1.54886500e-39 ... 9.76766125e-35\n",
            "   8.13129276e-35 1.12278775e-22]\n",
            "  [3.59711428e-31 2.04569958e-40 2.64060683e-41 ... 1.18685585e-36\n",
            "   9.32249116e-37 5.79237015e-24]\n",
            "  ...\n",
            "  [1.15239938e-37 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "   0.00000000e+00 2.13632672e-28]\n",
            "  [6.18263178e-36 1.40129846e-45 0.00000000e+00 ... 1.12103877e-44\n",
            "   7.14662217e-44 4.79019915e-27]\n",
            "  [4.58455548e-24 7.62480721e-29 1.85069154e-29 ... 5.43026155e-28\n",
            "   1.93461801e-27 1.42692507e-16]]]\n",
            "Epoch:  2\n",
            "Step: 0,         Loss: [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 5.5805310e-41\n",
            "   1.0039743e-40 1.7108469e-28]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 6.2899613e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.1673517e-40]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.6780637e-35]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 4.7520729e-33]\n",
            "  [1.0206199e-35 6.5959119e-42 4.9886225e-43 ... 9.9542108e-32\n",
            "   1.9748644e-31 5.8537314e-21]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.8651283e-42\n",
            "   3.0548307e-42 1.4618520e-29]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.2153550e-39]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 7.1480235e-42]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.3477174e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 5.8383000e-36]\n",
            "  [1.9416392e-41 0.0000000e+00 0.0000000e+00 ... 1.2709436e-34\n",
            "   3.2947915e-34 8.8926605e-23]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.6441930e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.2930514e-43]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.5818870e-40]\n",
            "  [4.3019863e-43 0.0000000e+00 0.0000000e+00 ... 1.1798576e-38\n",
            "   3.4783140e-38 2.2232713e-25]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 9.2531381e-40]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.3291224e-31]]]\n",
            "Epoch:  3\n",
            "Step: 0,         Loss: [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 5.5810915e-41\n",
            "   1.0040724e-40 1.7109513e-28]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 6.2905370e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.1674638e-40]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.6782175e-35]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 4.7524715e-33]\n",
            "  [1.0206899e-35 6.5959119e-42 4.9886225e-43 ... 9.9551229e-32\n",
            "   1.9750148e-31 5.8540210e-21]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.8651283e-42\n",
            "   3.0562320e-42 1.4619301e-29]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.2155512e-39]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 7.1494248e-42]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.3478615e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 5.8389234e-36]\n",
            "  [1.9417793e-41 0.0000000e+00 0.0000000e+00 ... 1.2710502e-34\n",
            "   3.2951182e-34 8.8932039e-23]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.6442682e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.2930514e-43]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.5821813e-40]\n",
            "  [4.3019863e-43 0.0000000e+00 0.0000000e+00 ... 1.1799656e-38\n",
            "   3.4786054e-38 2.2233813e-25]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 9.2534884e-40]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.3292137e-31]]]\n",
            "Epoch:  4\n",
            "Step: 0,         Loss: [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 5.5813718e-41\n",
            "   1.0041425e-40 1.7110427e-28]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 6.2910168e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.1675619e-40]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.6783969e-35]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 4.7529432e-33]\n",
            "  [1.0207599e-35 6.5959119e-42 4.9886225e-43 ... 9.9559575e-32\n",
            "   1.9751956e-31 5.8544011e-21]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.8651283e-42\n",
            "   3.0562320e-42 1.4620304e-29]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.2158455e-39]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 7.1494248e-42]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.3480361e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 5.8395910e-36]\n",
            "  [1.9419194e-41 0.0000000e+00 0.0000000e+00 ... 1.2712054e-34\n",
            "   3.2955207e-34 8.8938823e-23]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.6444061e-38]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.2930514e-43]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 3.5826297e-40]\n",
            "  [4.3019863e-43 0.0000000e+00 0.0000000e+00 ... 1.1801007e-38\n",
            "   3.4790303e-38 2.2235598e-25]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 9.2541330e-40]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  ...\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 0.0000000e+00]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "   0.0000000e+00 1.3293253e-31]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
